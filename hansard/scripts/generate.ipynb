{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28kl8eh0pro5"
      },
      "source": [
        "# Generate\r\n",
        "\r\n",
        "This is a notebook for generating text with the pretrained HansardGPT2 model. This should be ran with a GPU in Google Colab.\r\n",
        "\r\n",
        "To ensure you are running with a GPU go to Runtime -> Change Runtime Type, and ensure GPU is selected as Hardware accelerator\r\n",
        "\r\n",
        "Downloads required module (HuggingFace transformers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCxPXpxBTKfV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7GcgDMnp4St"
      },
      "source": [
        "Downloads model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPt6V5_HTQT9"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\r\n",
        "import torch\r\n",
        "\r\n",
        "model = GPT2LMHeadModel.from_pretrained(\"CallumRai/HansardGPT2\")\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"CallumRai/HansardGPT2\")\r\n",
        "\r\n",
        "device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi4KkIRxp7-2"
      },
      "source": [
        "By adding starting text to ```prompt += \"\"```, will print three utterances generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofkTfaXKTz00"
      },
      "source": [
        "prompt = \"<|startoftext|> \"\r\n",
        "prompt += \"\"\r\n",
        "\r\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\r\n",
        "generated = generated.to(device)\r\n",
        "\r\n",
        "sample_outputs = model.generate(\r\n",
        "                                generated,  \r\n",
        "                                do_sample=True,\r\n",
        "                                top_k=50,\r\n",
        "                                max_length = 300,\r\n",
        "                                top_p=0.95,\r\n",
        "                                num_return_sequences=3\r\n",
        "                                )\r\n",
        "\r\n",
        "for i, sample_output in enumerate(sample_outputs):\r\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}